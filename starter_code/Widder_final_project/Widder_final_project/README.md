# Determining Support/Opposition Using a Recursive Neural Network

## Dependencies

To run the network, you will need numpy, tensorflow, and nltk (in addition to standard python libraries). You should use python 3.6. 
The network works best when run on a computer with a GPU.

## Code

### rnn.py

This file holds the main RNN Model. Running `python3 rnn.py` will run the full training and testing process over a maximum of 40 epochs. Hyperparameters are defined in a Config object at the top of the file. As it stands, running `python3 rnn.py` will train the model on one debate only (examples in `parsed_006.txt`). This can take several hours. To enable early stopping (after 2 epochs of minimal improvement), uncomment the `break` on line 376. See comments in `tree.py` for instructions on how to change datasets.

#### Key Methods
* The `load_data()` function loads the training, test, and development sets of data using a helper function defined in `tree.py` (discussed below).
* `add_model()` computes basic forward computation at one node.
* `add_projections()` computes the weighted output activation
* `predict()` makes predictions using the model
* `run_epoch()` carries out training over one epoch
* `train()` carries out training over the max number of epochs
* `load_w2v()` retrieves word embedddings from the specified GloVe file - 50-D or 100-D


### tree.py

This file holds the Node and Tree classes. These take string parse trees and create Tree objects that are usable by the network.

#### Key Methods & Classes
* `Node` class holds the information stored at each node in the tree: the word, activation, children, parent, and label.
* `Tree` class holds information for the entire tree: the root node and the labels in the tree
* `parse()` class is used to initialize a tree from a string parse tree generated by the Stanford Parser
* `simplified_data()` is used to load and segment trees from text files into training, dev, and test sets. Modify the call to `load_trees()` as described in the comments to change the dataset used as input to the network. 

### utils.py

This file holds the Vocab class, as well as several helper functions not used in this project (leftover from NLP HW4).

#### Key Methods & Classes
* `Vocab` class stores all words used in the input data. This is used in `load_data()` in `rnn.py` when embeddings are added. For words in the input data that are in the GloVe vocabulary, the GloVe vector for that word is added to the embedding matrix. For unincluded words, a random small embedding vector is added instead.

### data_util.py

This file was used in the preparation of input data - parsing, filtering, labeling, etc. It does not need to be used when running the model (and in fact cannot be run without a downloaded copy of the Stanford Parser), but is included for general reference on the process by which the files in the `project_data` directory were produced.

### Project_data directory

This holds the input data for the model. 
* `parsed_train.txt`, `parsed_dev.txt`, and `parsed_test.txt` are all sentences filtered by their presence in Mohit Iyyer's ideologically biased set of Convote sentences. In total, these files contain about 6000 trees.
* `parsed_extreme.txt` contains about 1000 trees from Convote that include the words "support" or "oppose". 
* `parsed_006.txt`, `parsed_013.txt`, and `parsed_016.txt` each hold a few hundred trees each drawn from one debate (regarding bill IDs 006, 013, and 016, respectively). The network performed best on the 006 set. You may have to change the numbers passed to `simplified_data()` in `rnn.py` `load_data()`, which specify the number of training, dev, and test trees to use, to reflect the smaller dataset size of these examples.